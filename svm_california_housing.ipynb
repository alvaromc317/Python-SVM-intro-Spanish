{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](plots/california_housing_intro.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El contenido de esta libreta está basado en el libro \n",
    "\n",
    "* [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/). Capítulo 2\n",
    "\n",
    "Trabajaremos con un conjunto de datos que contiene información acerca de las casas en distintos distritos del estado de California. Entre otras variables, tendremos la población, los ingresos medianos del barrio, número medio de habitaciones por casa, etc. Con esta información, nuestro objetivo será intentar predecir el precio mediano de las casas de un barrio. Empecemos por cargar los paquetes básicos y acceder al conjunto de datos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de datos\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(5) # Los resultados del notebook serán los mismos en cada ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = pd.read_csv('data/housing.csv')\n",
    "\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos un conjunto de datos con 11 variables. Una, `median_house_value` es la variable respuesta que queremos predecir, y de las otras 10, tenemos 1 categórca (`ocean_proximity`) y 9 numéricas. Ya que tenemos la `longitud` y `latitud` de los barrios del conjunto de datos, representemoslos gráficamente de forma que el color dependa de la variable respuesta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualización inicial\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n",
    "             s=housing[\"population\"]/100, label=\"population\", figsize=(10,7),\n",
    "             c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n",
    "             sharex=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver claramente que el precio es mucho más alto en la costa, alrededor de Los Ángeles y San Francisco. Veamos los scatterplots entre la variable respuesta y las demás variables, para estudiar cuales pueden estar más relacionadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names = [var for var in housing.columns if var != 'median_house_value']\n",
    "\n",
    "plt.figure(figsize=(19,10))\n",
    "for idx, var in enumerate(var_names):\n",
    "    plt.subplot(3, 4, idx+1)\n",
    "    plt.scatter(housing[var], housing['median_house_value'])\n",
    "    plt.title(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que la variable más relacionada con la respuesta parece ser el `median_income`. Veamos también las correlaciones entre la variable respuesta y las demás variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que efectivamente, la variable más correlacionada es `median_income`, seguida (con una correlación negativa) por `bedrooms_per_household`. El que las correlaciones sean tan bajas nos hace prever que los resultados de nuestros modelos de predicción no serán muy buenos. A continuación veamos la relación entre la variable categórica `ocean_proximity` y la respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing['ocean_proximity'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotnine import *\n",
    "\n",
    "(ggplot(data=housing, mapping=aes(x='ocean_proximity', y='median_house_value')) + geom_boxplot() + theme_light()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='D12828'> Ejercicio: </font>\n",
    "1. Obtén un histograma de la variable respuesta `median_house_value`. Pista: usa la función `plt.hist`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesado\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='D12828'> Ejercicio: </font>\n",
    "1. Divide el dataframe `housing` en la parte de los predictores `X`, y la respuesta `y`. Pista: la variable respuesta es `median_house_value`\n",
    "2. Divide `X` e `y` en un conjunto de train y otro de test de forma que el `train_size` sea de 5000 observaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estandarización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos visto que cuando tratamos con variables numéricas, normalmente es recomendable estandarizarlas para que la escala de las unidades no afecten a la capacidad de hacer predicciones de los algoritmos, pero solo tiene sentido estandarizar las variables que sean numéricas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='D12828'> Ejercicio: </font>\n",
    "1. Crea un dataframe llamado `X_num` solo con las variables numéricas de `X_train` (elimina `ocean_proximity`)\n",
    "2. Estandariza el conjunto `X_num` usando la función `sklearn.preprocessing.StandardScaler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ona hot encoding\n",
    "\n",
    "Por otro lado, tenemos la variable categórica `ocean_proximity`. La mayoría de algoritmos de sklearn no aceptan variables que no sean numéricas, por lo que para incorporar estas variables categóricas es necesario codificarlas. Para ello usaremos la conocida como __one-hot encoding__, utilizada en prácticamente todos los modelos de ML, directa o indirectamente, para incorporar variables categóricas:\n",
    "\n",
    "![image.png](plots/one_hot_encoding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La idea es sencilla: dada una variable categórica con k clases, se construyen k variables numéricas que codifican con 1 o 0 cada clase. Esta codificación ya está programada en sklearn, veamos como usarla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "X_cat = X_train[[\"ocean_proximity\"]]\n",
    "\n",
    "cat_encoder = OneHotEncoder()\n",
    "X_cat_onehot = cat_encoder.fit_transform(X_cat)\n",
    "\n",
    "print(X_cat_onehot.toarray()[0:5, 0:5], '\\n')\n",
    "\n",
    "print(cat_encoder.categories_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesado con pipelines\n",
    "\n",
    "Ahora estamos haciendo sobre una parte de las variables (las numéricas) un tipo de preprocesado, y sobre otra parte de las variables (la categórica) otro preprocesado. Veamos cómo podemos unificar todo este proceso usando un tipo especial de pipeline llamado `ColumnTransformer`, que nos permite aplicar funciones diferentes a columnas diferentes de los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_attribs = [var for var in X_train if var != 'ocean_proximity']\n",
    "cat_attribs = ['ocean_proximity']\n",
    "\n",
    "print(num_attribs, '\\n')\n",
    "print(cat_attribs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    # Preprocesado sobre variables numéricas\n",
    "    ('num', StandardScaler(), num_attribs),\n",
    "    \n",
    "    # Preprocesado sobre variables categóricas\n",
    "    ('cat', OneHotEncoder(), cat_attribs),\n",
    "])\n",
    "\n",
    "X_train_preproces = full_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La ventaja principal de construir un pipeline de este estilo viene sobre todo cuando necesitamos llevar a cabo el mismo preprocesado más de una vez, por ejemplo, sobre el conjunto de datos de test, o al recibir nuevas observaciones sobre las que queramos hacer predicciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='D12828'> Ejercicio: </font>\n",
    "1. Lleva a cabo el preprocesado del conjunto de datos `X_test` y almacena el resultado en `X_test_preproces`. Recuerda que queremos escalar los datos del test en base a la información del train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construcción del modelo\n",
    "___\n",
    "\n",
    "Ahora que ya tenemos el conjunto de datos preparado, es hora de construir el modelo. Dado que la variable respuesta es numérica, se trata de un problema de regresión, por lo que construiremos una SVM de regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# Construcción del modelo\n",
    "svr_lineal = SVR(kernel='linear')\n",
    "svr_lineal.fit(X_train_preproces, y_train)\n",
    "\n",
    "# Predicción \n",
    "prediccion = svr_lineal.predict(X_test_preproces)\n",
    "\n",
    "print(f'Predicción {np.round(prediccion[11:14], 0)}')\n",
    "print(f'Valor      {y_test[11:14].values}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos una métrica del error de este modelo. Para ello usaremos el `mean_squared_error`, o error cuadrático medio. Este error calcula la diferencia entre las predicciones y el verdadero valor de las observaciones, y después las eleva al cuadrado. Al elevarlas al cuadrado se consigue que todos los errores sean números positivos, y por último hace la media de los errores. Es muy habitual tomar después la raiz de este error,   para devolver el error a las unidades originales (es el llamado root mean squared error), ya que es más sencillo de interpretar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(y_test, prediccion)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f'RMSE: {rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='D12828'> Ejercicio: </font>\n",
    "1. Intentemos mejorar este modelo. Utiliza la función de `sklearn.model_selection.GridSearchCV` para optimizar el tipo de red. En el mallado, incluye una red lineal y optimiza `C`, y una red rbf y optimiza `C` y `gamma`. Utiliza como scoring `neg_mean_squared_error`\n",
    "2. Obtén el RMSE de las predicciones en la parte de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final\n",
    "___\n",
    "Con esto termina nuestro análisis del conjunto de datos de MNIST.\n",
    "\n",
    "![image.png](plots/fin_svm_2.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
