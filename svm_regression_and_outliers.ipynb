{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](plots/one_class_svm.png)\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decimos que un punto es un outlier si tiene un valor anómalo comparado con el resto de puntos del conjunto de datos. Un problema típico en estadística es el de buscar estos outliers para estudiarlos más a fondo o eliminarlos del conjunto de datos. Las SVM también pueden ayudarnos en este trabajo, en concreto, las conocidas como __One class SVM__.\n",
    "\n",
    "Estas SVM se llaman One class porque se entrenan en conjnutos de datos en los que solo tenemos una clase: los datos que no son outliers. La SVM aprenderá cómo se distribuyen estos datos y construirá un hiperplano en el que todos los puntos normales estén en el mismo lado. De esta forma, los puntos extraños (los outliers) estarán en el otro lado del hiperplano.\n",
    "\n",
    "Veamos un ejemplo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En las libretas anteriores hemos visto cómo trabajar con las SVM en problemas de clasificación, en los que la variable respuesta es categórica. Pero también es posible usarlas en problemas de regresión. En este tipo de problemas, en lugar de buscar un hiperplano separador de margen máximo (objetivo de la clasificación) lo que buscamos es __un hiperplano tal que los margenes contengan al mayor número posible de puntos__. Aún así, la idea detrás de las SVM en regresión sigue siendo la misma: aprovechar el __kernel trick__ para \"proyectar\" el conjunto de datos a un espacio de dimensión más alta donde tengan un comportamiento lineal. \n",
    "\n",
    "Veamos un ejemplo con un conjunto de datos sintético con dos variables, una predictora y una respuesta con una relación no lineal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.svm import OneClassSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/data_outlier.csv')\n",
    "\n",
    "plt.scatter(data['V1'], data['V2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que la mayoría de puntos están concentrados en el centro, pero unos pocos están más alejados. Esos puntos son outliers, comparados con el resto del conjunto. Intentemos detectarlos usando una One Class SVM. En primer lugar, estandarizamos el conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (data - data.mean()) / data.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora entrenemos una One Class SVM y obtengamos las predicciones asociadas al conjunto de datos. Hay un cambio importante en los parámetros de las One Class SVM en comparación con las SVM de clasificación y de regresión, y es el uso del parámetro `nu` $\\nu$. Este parámetro:\n",
    "\n",
    "* Establece una cota superior de la fracción de puntos que creemos que son outliers, \n",
    "* Establece una cota inferior al número de puntos usados como vectores soporte.\n",
    "\n",
    "Es por esto que suele darse al parámetro `nu` el valor del porcentaje aproximado de outliers que creemos que hay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = OneClassSVM(kernel='rbf', gamma=0.001, nu=0.015)\n",
    "\n",
    "final_prediction = svm.fit_predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recuperamos dos funciones definidas al inicio del curso, que nos permitirán representar gráficamente la frontera de decisión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_plot_svm(X, y):\n",
    "    \"\"\"\n",
    "    X: matriz de dimensiones [n_obs, 2]\n",
    "    y: vector binario con valores 0 o 1\n",
    "    Representación del scatterplot de las variables de X en base a las categorías de y\n",
    "    \"\"\"\n",
    "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")    # Pintar con triangulos verdes los puntos de la categoría 1\n",
    "    plt.plot(X[:, 0][y==-1], X[:, 1][y==-1], \"bs\")    # Pintar con cuadrados azules los puntos de la categoría 0\n",
    "    plt.grid(True, which='both')  \n",
    "\n",
    "def plot_predictions(svm_clf, axes):\n",
    "    \"\"\"\n",
    "    svm_clf: Pipeline con una SVM final\n",
    "    axes: ejes en los que representar el hiperplano\n",
    "    Función que representa gráficamente el hiperplano separador generado por la svm\n",
    "    \"\"\"\n",
    "    x0s = np.linspace(axes[0], axes[1], 100)                      # Generamos puntos en eje horizontal\n",
    "    x1s = np.linspace(axes[2], axes[3], 100)                      # Generamos puntos en eje vertical\n",
    "    x0, x1 = np.meshgrid(x0s, x1s)                                # Hacemos mallado de puntos\n",
    "    X = np.c_[x0.ravel(), x1.ravel()]                             # Matriz que contiene 2 columnas, con cada par de valores \n",
    "    y_pred = svm_clf.predict(X).reshape(x0.shape)                 # Predicciones de la svm\n",
    "    y_decision = svm_clf.decision_function(X).reshape(x0.shape)   # Valor de la función de decisión\n",
    "    plt.contourf(x0, x1, y_pred, cmap=plt.cm.brg, alpha=0.2)      # Añadimos contorno en base a las predicciones\n",
    "    plt.contourf(x0, x1, y_decision, cmap=plt.cm.brg, alpha=0.1)  # Añadimos lineas de nivel en base al valor de la función de decisión (hiperplano)\n",
    "    svs = svm_clf.support_vectors_                                    # Vectores soporte\n",
    "    plt.scatter(svs[:, 0], svs[:, 1], s=180, facecolors='#FFAAAA')# Pintar area alrededor de vectores soporte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(svm, axes=[-4, 4, -4, 4])\n",
    "scatter_plot_svm(data.values, final_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](plots/svm_regresion.png)\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-6.0, 6.0, 0.1)\n",
    "\n",
    "y = 1*(x**3) + 2*(x**2) + 1*x + 3\n",
    "y_noise = 20 * np.random.normal(size=x.size)\n",
    "\n",
    "ydata = y + y_noise\n",
    "\n",
    "x = x.reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_scatter(x,y):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(x, ydata,  'bo')\n",
    "    plt.ylabel('Variable dependiente')\n",
    "    plt.xlabel('Variable independiente')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_scatter(x,ydata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos cómo entrenar una SVM con un kernel lineal en primer lugar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_lineal = SVR(kernel='linear')\n",
    "svr_lineal.fit(x, ydata)\n",
    "final_prediction = svr_lineal.predict(x)\n",
    "\n",
    "custom_scatter(x,ydata)\n",
    "plt.plot(x, final_prediction, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que nuestra predicción (en rojo) no se aproxima nada bien al comportamiento no lineal de este conjunto de datos.\n",
    "\n",
    "### <font color='D12828'> Ejercicio: </font>\n",
    "* Prueba a entrenar una SVR con kernel polinómico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final\n",
    "___\n",
    "Con esto termina nuestra introducción a las máquinas de vector soporte. ¡Espero que hayais disfrutado del curso!\n",
    "\n",
    "![image.png](plots/fin_svm_2.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
